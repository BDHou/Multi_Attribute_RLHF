# -*- coding: utf-8 -*-
"""testRM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DJEnFS5Dhe5H_IRsFlIpyk6dDZA8g6Pr

https://huggingface.co/weqweasdas/hh_rlhf_rm_open_llama_3b
"""

# A0: Install deps (Colab)
# !pip -q install datasets transformers sentence-transformers textstat accelerate

# A1: Setup & config
import os, math, random, re, json
from typing import List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score
from sklearn.decomposition import PCA
from tqdm.auto import tqdm
device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# A2: Load dataset
DATASET_NAME    = "Dahoas/full-hh-rlhf"
TRAIN_SIZE      = None     # set None for full train
TEST_SIZE       = None     # set None for full test
ds = load_dataset(DATASET_NAME)
train_split = ds["train"]
test_split  = ds["test"]

if TRAIN_SIZE is not None:
    train_split = train_split.select(range(min(TRAIN_SIZE, len(train_split))))
if TEST_SIZE is not None:
    test_split  = test_split.select(range(min(TEST_SIZE, len(test_split))))

print("Example keys:", train_split[0].keys())

# New Cell A: switch backbone to Open-LLaMA-3B (or Gemma-2B RM as an alternative)
from transformers import AutoTokenizer, AutoModelForCausalLM

BASE_MODEL = "openlm-research/open_llama_3b"  # alt: "google/gemma-2-2b" (requires license), or weqweasdas/RM-Gemma-2B for inference-only
bf16_ok = torch.cuda.is_available() and torch.cuda.is_bf16_supported()

tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

backbone = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL,
    torch_dtype=torch.bfloat16 if bf16_ok else torch.float32,
    device_map=None
).to(device)

hidden_size = backbone.config.hidden_size
print("Loaded:", BASE_MODEL, "| hidden_size:", hidden_size)

# New Cell B: SFT dataset & packer (chosen-only, 1024 token packing)
from torch.utils.data import Dataset, DataLoader
from itertools import chain

PACK_LEN = 1024

def format_sft(prompt:str, chosen:str):
    return f"###Human: {prompt}\n\n###Assistant: {chosen}{tokenizer.eos_token}"

class SFTStreamDataset(Dataset):
    def __init__(self, split, pack_len=PACK_LEN, max_examples=None):
        self.pack_len = pack_len
        texts = []
        n = len(split) if max_examples is None else min(len(split), max_examples)
        for i in range(n):
            item = split[i]
            texts.append(format_sft(item["prompt"], item["chosen"]))
        # tokenize all then pack into chunks of pack_len
        enc = tokenizer(texts, add_special_tokens=False, return_attention_mask=False)
        ids = list(chain.from_iterable(enc["input_ids"]))
        # cut into chunks
        self.chunks = [ids[i:i+pack_len] for i in range(0, len(ids), pack_len)]
        if len(self.chunks[-1]) < 32:  # drop a tiny tail
            self.chunks.pop()

    def __len__(self): return len(self.chunks)
    def __getitem__(self, i):
        x = self.chunks[i]
        inp = torch.tensor(x, dtype=torch.long)
        att = torch.ones_like(inp)
        return {"input_ids": inp, "attention_mask": att, "labels": inp.clone()}

def sft_collate(batch):
    # already packed same length
    ids  = torch.nn.utils.rnn.pad_sequence([b["input_ids"]  for b in batch], batch_first=True, padding_value=tokenizer.pad_token_id)
    mask = torch.nn.utils.rnn.pad_sequence([b["attention_mask"] for b in batch], batch_first=True, padding_value=0)
    labs = torch.nn.utils.rnn.pad_sequence([b["labels"] for b in batch], batch_first=True, padding_value=-100)
    return {"input_ids": ids, "attention_mask": mask, "labels": labs}

SFT_BATCH = 2  # per-GPU; increase if memory allows
sft_train = SFTStreamDataset(train_split)   # uses your existing train_split
sft_loader = DataLoader(sft_train, batch_size=SFT_BATCH, shuffle=True, collate_fn=sft_collate, num_workers=2)
print("SFT chunks:", len(sft_train))

# New Cell C (patched): SFT train loop with bf16-safe AMP (no GradScaler on bf16)
from transformers import get_linear_schedule_with_warmup

SFT_EPOCHS = 2
SFT_LR = 2e-5
SFT_WARMUP = max(100, int(0.03 * len(sft_loader) * SFT_EPOCHS))

opt = torch.optim.AdamW(backbone.parameters(), lr=SFT_LR, weight_decay=0.01)
sch = get_linear_schedule_with_warmup(
    opt, num_warmup_steps=SFT_WARMUP, num_training_steps=len(sft_loader)*SFT_EPOCHS
)

USE_BF16 = (device == "cuda") and torch.cuda.is_bf16_supported()
USE_FP16 = (device == "cuda") and (not USE_BF16)
print(f"[SFT] USE_BF16={USE_BF16}, USE_FP16={USE_FP16}")

if USE_FP16:
    scaler = torch.amp.GradScaler("cuda")
else:
    scaler = None  # bf16 / fp32 do not use

backbone.train()
print("Start SFT...")
for ep in range(1, SFT_EPOCHS+1):
    pbar = tqdm(sft_loader, desc=f"SFT Epoch {ep}")
    run = 0.0
    for b in pbar:
        ids = b["input_ids"].to(device)
        att = b["attention_mask"].to(device)
        labs = b["labels"].to(device)

        with torch.autocast(
            device_type="cuda",
            dtype=(torch.bfloat16 if USE_BF16 else torch.float16),
            enabled=(device=="cuda")
        ):
            out = backbone(input_ids=ids, attention_mask=att, labels=labs)
            loss = out.loss

        opt.zero_grad(set_to_none=True)
        if scaler is not None:  # FP16
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(backbone.parameters(), 1.0)
            scaler.step(opt); scaler.update()
        else:  # BF16/FP32
            loss.backward()
            torch.nn.utils.clip_grad_norm_(backbone.parameters(), 1.0)
            opt.step()
        sch.step()

        run += float(loss.item()) * ids.size(0)
        pbar.set_postfix(loss=f"{loss.item():.4f}")
    print(f"SFT Epoch {ep} | avg loss {(run/len(sft_train)):.4f}")

"""https://docs.pytorch.org/docs/stable/notes/cuda.html#environment-variables"""

# New Cell D: RM wrapper with answer-only pooling head
class LMRewardModel(nn.Module):
    def __init__(self, backbone, hidden_size):
        super().__init__()
        self.backbone = backbone
        self.value_head = nn.Sequential(
            nn.LayerNorm(hidden_size),
            nn.Linear(hidden_size, hidden_size*2),
            nn.GELU(),
            nn.Linear(hidden_size*2, 1)
        )

    def answer_pool_score(self, joint_ids, joint_mask, ans_len, length_beta: float = 0.0):
        out = self.backbone(
            input_ids=joint_ids,
            attention_mask=joint_mask,
            output_hidden_states=True,
            use_cache=False
        )
        H = out.hidden_states[-1]      # (B, T, H)
        seq_len = joint_mask.sum(dim=1)  # (B,)
        reps = []
        B = H.size(0)
        for i in range(B):
            L  = int(seq_len[i].item())
            aL = int(ans_len[i].item())
            aL = max(1, min(aL, L))
            start = L - aL
            reps.append(H[i, start:L, :].mean(dim=0))
        Z = torch.stack(reps, dim=0)  # (B, H)
        s = self.value_head(Z).squeeze(-1)
        if length_beta and float(length_beta) > 0:
            a = ans_len.to(s.dtype)
            s = s - length_beta * torch.log1p(a)
        return s

# New Cell E: RM dataset (strict 512) & collator
MAX_LEN_RM = 512

def fmt_joint(prompt, answer):
    return f"###Human: {prompt}\n\n###Assistant: {answer}"

class RMDataset(Dataset):
    def __init__(self, split, max_len=MAX_LEN_RM, drop_long=True):
        self.data = split
        self.max_len = max_len
        self.drop_long = drop_long

    def __len__(self): return len(self.data)

    def _enc(self, text):
        return tokenizer(text, add_special_tokens=False, truncation=False, padding=False)

    def __getitem__(self, i):
        item = self.data[i]
        ch = self._enc(fmt_joint(item["prompt"], item["chosen"]))
        rj = self._enc(fmt_joint(item["prompt"], item["rejected"]))
        return {
            "ch_ids": ch["input_ids"], "rj_ids": rj["input_ids"]
        }

def _left_crop_to(ids, max_len):
    return ids[-max_len:] if len(ids) > max_len else ids

def rm_collate(batch):
    ch_seqs, rj_seqs, chL, rjL = [], [], [], []
    for b in batch:
        ch = _left_crop_to(b["ch_ids"], MAX_LEN_RM)
        rj = _left_crop_to(b["rj_ids"], MAX_LEN_RM)
        ch_seqs.append(ch); rj_seqs.append(rj)
        # 粗略估计答案长度（以 joint 的末尾作为回答尾部）
        chL.append(min(len(ch), MAX_LEN_RM//2) if len(ch)>0 else 1)  # 保守：下游 mean-pool 末尾 aL
        rjL.append(min(len(rj), MAX_LEN_RM//2) if len(rj)>0 else 1)

    def pad(seqs):
        L = max(len(s) for s in seqs)
        ids = torch.full((len(seqs), L), tokenizer.pad_token_id, dtype=torch.long)
        msk = torch.zeros((len(seqs), L), dtype=torch.long)
        for i,s in enumerate(seqs):
            ids[i,:len(s)] = torch.tensor(s, dtype=torch.long)
            msk[i,:len(s)] = 1
        return ids, msk

    ch_ids, ch_m = pad(ch_seqs)
    rj_ids, rj_m = pad(rj_seqs)
    return {
        "ch_ids": ch_ids, "ch_m": ch_m, "ch_aL": torch.tensor(chL, dtype=torch.long),
        "rj_ids": rj_ids, "rj_m": rj_m, "rj_aL": torch.tensor(rjL, dtype=torch.long),
    }

rm_train = RMDataset(train_split)
rm_val   = RMDataset(test_split)
RM_BS = 8
tr_loader = DataLoader(rm_train, batch_size=RM_BS, shuffle=True,  collate_fn=rm_collate, num_workers=2)
va_loader = DataLoader(rm_val,   batch_size=RM_BS, shuffle=False, collate_fn=rm_collate, num_workers=2)

# New Cell F (patched): RM train loop with bf16-safe AMP
from sklearn.metrics import roc_auc_score
from transformers import get_linear_schedule_with_warmup

def bt_loss(sp, sn):
    return F.softplus(-(sp - sn)).mean()

model = LMRewardModel(backbone, hidden_size).to(device)

RM_EPOCHS = 1
RM_LR = 5e-6
USE_LISTWISE = True
LENGTH_BETA = 0.004

opt = torch.optim.AdamW(
    filter(lambda p: p.requires_grad, model.parameters()),
    lr=RM_LR, weight_decay=5e-4
)
sch = get_linear_schedule_with_warmup(
    opt,
    num_warmup_steps=max(50, int(0.03*len(tr_loader))),
    num_training_steps=len(tr_loader)*RM_EPOCHS
)

USE_BF16 = (device == "cuda") and torch.cuda.is_bf16_supported()
USE_FP16 = (device == "cuda") and (not USE_BF16)
print(f"[RM] USE_BF16={USE_BF16}, USE_FP16={USE_FP16}")
scaler = torch.amp.GradScaler("cuda") if USE_FP16 else None

def eval_pair(model, loader):
    model.eval()
    pos, neg = [], []
    with torch.no_grad():
        for b in loader:
            ch_ids = b["ch_ids"].to(device); ch_m = b["ch_m"].to(device); chL = b["ch_aL"].to(device)
            rj_ids = b["rj_ids"].to(device); rj_m = b["rj_m"].to(device); rjL = b["rj_aL"].to(device)
            sp = model.answer_pool_score(ch_ids, ch_m, chL, LENGTH_BETA)
            sn = model.answer_pool_score(rj_ids, rj_m, rjL, LENGTH_BETA)
            pos.append(sp.cpu()); neg.append(sn.cpu())
    sp = torch.cat(pos).numpy(); sn = torch.cat(neg).numpy()
    acc = float((sp > sn).mean())
    s = 1/(1+np.exp(-np.concatenate([sp-(sp+sn)/2, sn-(sp+sn)/2])))
    y = np.concatenate([np.ones_like(sp), np.zeros_like(sn)])
    try: auc = float(roc_auc_score(y, s))
    except: auc = float('nan')
    return acc, auc

print("Start RM training...")
for ep in range(1, RM_EPOCHS+1):
    model.train()
    pbar = tqdm(tr_loader, desc=f"RM Epoch {ep}")
    run = 0.0
    for b in pbar:
        ch_ids = b["ch_ids"].to(device); ch_m = b["ch_m"].to(device); chL = b["ch_aL"].to(device)
        rj_ids = b["rj_ids"].to(device); rj_m = b["rj_m"].to(device); rjL = b["rj_aL"].to(device)

        with torch.autocast(
            device_type="cuda",
            dtype=(torch.bfloat16 if USE_BF16 else torch.float16),
            enabled=(device=="cuda")
        ):
            sp = model.answer_pool_score(ch_ids, ch_m, chL, LENGTH_BETA)
            sn = model.answer_pool_score(rj_ids, rj_m, rjL, LENGTH_BETA)

            if USE_LISTWISE and sp.numel() > 1:
                S = sp[:, None] - sn[None, :]
                diag_loss = F.softplus(-torch.diagonal(S)).mean()
                off_loss  = 0.25 * F.softplus(-S).mean()
                loss = diag_loss + off_loss
            else:
                loss = bt_loss(sp, sn)

        opt.zero_grad(set_to_none=True)
        if scaler is not None:  # FP16
            scaler.scale(loss).backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            scaler.step(opt); scaler.update()
        else:                   # BF16/FP32
            loss.backward()
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            opt.step()
        sch.step()

        run += float(loss.item()) * ch_ids.size(0)
        pbar.set_postfix(loss=f"{loss.item():.4f}")

    tr_acc, tr_auc = eval_pair(model, tr_loader)
    va_acc, va_auc = eval_pair(model, va_loader)
    print(f"Epoch {ep:02d} | train_loss {(run/len(rm_train)):.4f} | train Acc {tr_acc:.3f} AUC {tr_auc:.3f} | val Acc {va_acc:.3f} AUC {va_auc:.3f}")
