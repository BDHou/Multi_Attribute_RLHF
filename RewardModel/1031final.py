# -*- coding: utf-8 -*-
"""1031final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ISCH9xGCOfsGFsMfhByvhr6f3KdtqSaM
"""

# # A0: Install deps (Colab)
# !pip -q install datasets transformers sentence-transformers textstat accelerate

# # A0: Install deps (Colab)
# !pip -q install datasets transformers sentence-transformers textstat accelerate

"""	•	先把管线跑通后再加量：TRAIN_SIZE=None, TEST_SIZE=None
	•	UNFREEZE_TOP_N = 4，MAX_LEN = 2048（卡允许）
	•	LENGTH_BETA 网格搜 0.002～0.006；EPOCHS=10~12、GRAD_ACC_STEPS=4
	•	仍差一点就把 mean-pool 换 attention-pool（learned pooling），或者补上 prompt 偏置归零（per-prompt 标量去均值）


没必要再在colab上小打小闹了。应该继续后续试验。
"""

# A1: Setup & config
import os, math, random, re, json
from typing import List, Tuple

import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.nn.functional as F

from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM
from torch.utils.data import Dataset, DataLoader
from sklearn.metrics import roc_auc_score
from sklearn.decomposition import PCA
from tqdm.auto import tqdm

device = "cuda" if torch.cuda.is_available() else "cpu"
print("Device:", device)

# Reproducibility
SEED = 42
random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)
if torch.cuda.is_available():
    torch.cuda.manual_seed_all(SEED)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# --- Config ---
MODEL_NAME      = "EleutherAI/gpt-neo-125M"
DATASET_NAME    = "Dahoas/full-hh-rlhf"
TRAIN_SIZE      = 6000     # set None for full train
TEST_SIZE       = 600      # set None for full test
MAX_LEN         = 1536     # safe joint length fed to model (≤ model max pos)
BATCH           = 8
FREEZE_BACKBONE = True
UNFREEZE_TOP_N  = 2        # unfreeze top-N transformer blocks
MLP_HIDDEN      = 2048
DROPOUT_P       = 0.10
LENGTH_BETA     = 0.003    # length debias on answer tokens (log1p(#ans_tokens))
EPOCHS          = 8
PATIENCE        = 3
GRAD_ACC_STEPS  = 2
TOPK_HARD       = 2        # hard negative blending in BT loss

# AutoMetrics compute cap (set None for full; big models = slow)
AUTOMETRICS_MAX = 1000

print({
    "MODEL": MODEL_NAME, "MAX_LEN": MAX_LEN, "BATCH": BATCH,
    "FREEZE_BACKBONE": FREEZE_BACKBONE, "UNFREEZE_TOP_N": UNFREEZE_TOP_N
})

# A2: Load dataset
ds = load_dataset(DATASET_NAME)
train_split = ds["train"]
test_split  = ds["test"]

if TRAIN_SIZE is not None:
    train_split = train_split.select(range(min(TRAIN_SIZE, len(train_split))))
if TEST_SIZE is not None:
    test_split  = test_split.select(range(min(TEST_SIZE, len(test_split))))

print("Example keys:", train_split[0].keys())  # expect: prompt, response, chosen, rejected

# A3: Tokenizer & backbone (+ silence tokenizer max_length warnings)
from transformers.utils import logging as hf_logging
hf_logging.enable_explicit_format()
hf_logging.set_verbosity_warning()

tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# silence tokenizer warnings: we'll crop ourselves downstream
tokenizer.model_max_length = 10**9
try:
    tokenizer.init_kwargs['model_max_length'] = 10**9
except Exception:
    pass
print(f"[patch] tokenizer.model_max_length -> {tokenizer.model_max_length} (we crop later)")

backbone = AutoModelForCausalLM.from_pretrained(MODEL_NAME)
backbone.config.pad_token_id = tokenizer.pad_token_id

hidden_size  = backbone.config.hidden_size
MODEL_MAXLEN = getattr(backbone.config, "max_position_embeddings", 2048)
SAFE_MAXLEN  = min(MODEL_MAXLEN, MAX_LEN)
print(f"hidden={hidden_size}, layers={getattr(backbone.config,'num_hidden_layers','?')}, "
      f"MODEL_MAXLEN={MODEL_MAXLEN}, SAFE_MAXLEN={SAFE_MAXLEN}")

# A4: Smart dataset & robust collate + sanity
def _enc(text: str):
    # no truncation here; we crop ourselves
    return tokenizer(text, add_special_tokens=False, truncation=False, return_tensors=None)

def smart_build_joint(prompt_ids: List[int], answer_ids: List[int], max_len: int):
    # Keep the full answer; take only the tail of the prompt to fit max_len
    p = torch.tensor(prompt_ids, dtype=torch.long)
    a = torch.tensor(answer_ids, dtype=torch.long)
    keep_prompt = max(0, max_len - len(a))
    if keep_prompt > 0:
        p_tail = p[-keep_prompt:] if len(p) > keep_prompt else p
        joint = torch.cat([p_tail, a], dim=0)
        ans_mask = torch.cat([torch.zeros_like(p_tail), torch.ones_like(a)], dim=0)
    else:
        joint = a[-max_len:]
        ans_mask = torch.ones_like(joint)
    attn = torch.ones_like(joint)
    return joint.tolist(), attn.tolist(), ans_mask.tolist()

class PairwiseSmartDataset(Dataset):
    def __init__(self, split, max_len=1024):
        self.data = split
        self.max_len = max_len
    def __len__(self): return len(self.data)
    def __getitem__(self, i):
        it = self.data[i]
        p_ids  = _enc("Human: " + it["prompt"])["input_ids"]
        ch_ids = _enc("Assistant: " + it["chosen"])["input_ids"]
        rj_ids = _enc("Assistant: " + it["rejected"])["input_ids"]
        ch_joint, ch_attn, ch_ansm = smart_build_joint(p_ids, ch_ids, self.max_len)
        rj_joint, rj_attn, rj_ansm = smart_build_joint(p_ids, rj_ids, self.max_len)
        return {
            "ch_ids": ch_joint, "ch_attn": ch_attn, "ch_ansm": ch_ansm,
            "rj_ids": rj_joint, "rj_attn": rj_attn, "rj_ansm": rj_ansm,
        }

def _pad(seqs, pad):
    L = max(len(x) for x in seqs)
    out = torch.full((len(seqs), L), pad, dtype=torch.long)
    for i, s in enumerate(seqs):
        out[i, :len(s)] = torch.tensor(s, dtype=torch.long)
    return out

def _right_crop(t, L):
    return t[:, -L:] if t.size(1) > L else t

def collate_pairwise(batch):
    out = {
        "ch_ids":  _pad([b["ch_ids"]  for b in batch], tokenizer.pad_token_id),
        "ch_attn": _pad([b["ch_attn"] for b in batch], 0),
        "ch_ansm": _pad([b["ch_ansm"] for b in batch], 0),
        "rj_ids":  _pad([b["rj_ids"]  for b in batch], tokenizer.pad_token_id),
        "rj_attn": _pad([b["rj_attn"] for b in batch], 0),
        "rj_ansm": _pad([b["rj_ansm"] for b in batch], 0),
    }
    # hard crop in collate
    for k in out:
        out[k] = _right_crop(out[k], SAFE_MAXLEN)
    return out

def guard_and_crop_before_forward(batch, limit: int):
    # second safety crop right before forward
    for k in ("ch_ids","ch_attn","ch_ansm","rj_ids","rj_attn","rj_ansm"):
        t = batch[k]
        if t.size(1) > limit:
            batch[k] = t[:, -limit:]
    return batch

# Build loaders (num_workers=0 avoids worker-state issues)
train_ds = PairwiseSmartDataset(train_split, max_len=SAFE_MAXLEN)
eval_ds  = PairwiseSmartDataset(test_split,  max_len=SAFE_MAXLEN)
tr_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  collate_fn=collate_pairwise, num_workers=0)
va_loader = DataLoader(eval_ds,  batch_size=BATCH, shuffle=False, collate_fn=collate_pairwise, num_workers=0)

# Sanity: check widths on a few mini-batches
with torch.no_grad():
    ch_max = rj_max = 0
    for i, b in enumerate(DataLoader(eval_ds, batch_size=16, collate_fn=collate_pairwise, num_workers=0)):
        ch_max = max(ch_max, b["ch_ids"].shape[1]); rj_max = max(rj_max, b["rj_ids"].shape[1])
        if i >= 10: break
print(f"[sanity] ch_max={ch_max}, rj_max={rj_max}, SAFE_MAXLEN={SAFE_MAXLEN}, MODEL_MAXLEN={MODEL_MAXLEN}")

# # A5-ATTN: Reward model with Answer-only Attention Pooling (mean/attn/mix)

# class AnswerPooling(nn.Module):
#     """
#     Learned attention pooling over answer tokens.
#     Attn = softmax((H Wq) · q / sqrt(H)), z = sum_i attn_i * H_i
#     """
#     def __init__(self, hidden_size):
#         super().__init__()
#         self.proj = nn.Linear(hidden_size, hidden_size, bias=False)
#         self.query = nn.Parameter(torch.randn(hidden_size))
#         nn.init.normal_(self.query, mean=0.0, std=hidden_size**-0.5)

#     def forward(self, H: torch.Tensor, ans_mask: torch.Tensor):
#         # H: [B,T,H], ans_mask: [B,T] (1 for answer tokens)
#         Q = self.proj(H)                                    # [B,T,H]
#         scores = torch.einsum("bth,h->bt", Q, self.query)   # [B,T]
#         # mask非答案/非可见位
#         scores = scores.masked_fill(ans_mask == 0, float("-inf"))
#         attn = torch.softmax(scores, dim=1)                 # [B,T]
#         z = torch.einsum("bt,bth->bh", attn, H)             # [B,H]
#         return z

# class LMRewardModel(nn.Module):
#     def __init__(self, backbone, hidden_size, mlp_hidden=2048, dropout_p=0.1,
#                  pool_type: str = "attn", mix_alpha_init: float = 0.0):
#         """
#         pool_type: "mean" / "attn" / "mix"
#         If "mix", will learn a sigmoid(alpha) to blend mean/attn.
#         """
#         super().__init__()
#         self.backbone = backbone
#         self.dropout  = nn.Dropout(dropout_p)
#         self.pool_type = pool_type
#         self.attn_pool = AnswerPooling(hidden_size)
#         self.mix_alpha = nn.Parameter(torch.tensor(mix_alpha_init))  # for "mix"

#         self.value_head = nn.Sequential(
#             nn.LayerNorm(hidden_size),
#             nn.Linear(hidden_size, mlp_hidden),
#             nn.GELU(),
#             nn.Dropout(dropout_p),
#             nn.Linear(mlp_hidden, 1),
#         )

#     def _last_hidden(self, input_ids, attention_mask):
#         out = self.backbone(
#             input_ids=input_ids,
#             attention_mask=attention_mask,
#             output_hidden_states=True,
#             use_cache=False
#         )
#         return out.hidden_states[-1]  # [B,T,H]

#     def _pool_answer(self, H: torch.Tensor, attn_mask: torch.Tensor, ans_mask: torch.Tensor):
#         # 仅在答案 token 上聚合；非答案/非可见位置权重为0
#         am = (ans_mask.float() * attn_mask.float())         # [B,T]
#         denom = am.sum(dim=1, keepdim=True).clamp_min(1.0)

#         # mean pooling
#         z_mean = (H * am.unsqueeze(-1)).sum(dim=1) / denom  # [B,H]

#         if self.pool_type == "mean":
#             return z_mean

#         # attn pooling
#         z_attn = self.attn_pool(H, (am > 0).to(H.dtype))    # [B,H]
#         if self.pool_type == "attn":
#             return z_attn

#         # mix pooling
#         w = torch.sigmoid(self.mix_alpha)
#         return w * z_attn + (1.0 - w) * z_mean

#     def reward_answer_only(self, input_ids, attention_mask, answer_mask, length_beta: float = 0.0):
#         H = self._last_hidden(input_ids, attention_mask)    # [B,T,H]
#         z = self._pool_answer(H, attention_mask, answer_mask)
#         z = self.dropout(z)
#         s = self.value_head(z).squeeze(-1)
#         if length_beta and length_beta > 0:
#             # 使用答案 token 数做长度去偏
#             denom = (answer_mask.float() * attention_mask.float()).sum(dim=1).clamp_min(1.0)
#             s = s - float(length_beta) * torch.log1p(denom)
#         return s

#     def answer_latent(self, input_ids, attention_mask, answer_mask):
#         H = self._last_hidden(input_ids, attention_mask)
#         z = self._pool_answer(H, attention_mask, answer_mask)
#         return z  # [B,H]

# # Instantiate with attention pooling by default
# model = LMRewardModel(
#     backbone, hidden_size,
#     mlp_hidden=MLP_HIDDEN, dropout_p=DROPOUT_P,
#     pool_type="attn",      # "mean" / "attn" / "mix"
#     mix_alpha_init=0.0     # only used if pool_type="mix"
# ).to(device)

# # Freeze & optional unfreeze top-N
# if FREEZE_BACKBONE:
#     for p in model.backbone.parameters(): p.requires_grad = False
#     if UNFREEZE_TOP_N > 0:
#         try:
#             blocks = model.backbone.transformer.h
#             for blk in blocks[-UNFREEZE_TOP_N:]:
#                 for p in blk.parameters(): p.requires_grad = True
#             print(f"Unfroze top {UNFREEZE_TOP_N} blocks.")
#         except Exception as e:
#             print("Unfreeze failed:", e)

# # Rebuild optimizer/scheduler/scaler (overwrite previous ones)
# param_groups = [{"params": model.value_head.parameters(), "lr": 1e-4}]
# # include attention pooling/mix alpha in head group
# head_extras = [*model.attn_pool.parameters()]
# if model.pool_type == "mix":
#     head_extras.append(model.mix_alpha)
# param_groups[0]["params"] = list(param_groups[0]["params"]) + head_extras

# if FREEZE_BACKBONE and UNFREEZE_TOP_N > 0:
#     top_params = []
#     for n,p in model.named_parameters():
#         if p.requires_grad and "transformer.h" in n:
#             top_params.append(p)
#     if top_params:
#         param_groups.append({"params": top_params, "lr": 5e-5})

# OPT    = torch.optim.AdamW(param_groups, weight_decay=5e-4)
# SCHED  = torch.optim.lr_scheduler.ReduceLROnPlateau(OPT, mode='min', factor=0.5, patience=2)
# SCALER = torch.amp.GradScaler("cuda", enabled=(device=="cuda"))
# USE_AMP = (device=="cuda")

# print(f"[pool] type={model.pool_type}")

# A5: Reward model (answer-only pooling + MLP + length debias)
class LMRewardModel(nn.Module):
    def __init__(self, backbone, hidden_size, mlp_hidden=2048, dropout_p=0.1):
        super().__init__()
        self.backbone = backbone
        self.dropout  = nn.Dropout(dropout_p)
        self.value_head = nn.Sequential(
            nn.LayerNorm(hidden_size),
            nn.Linear(hidden_size, mlp_hidden),
            nn.GELU(),
            nn.Dropout(dropout_p),
            nn.Linear(mlp_hidden, 1),
        )

    def _last_hidden(self, input_ids, attention_mask):
        out = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            output_hidden_states=True,
            use_cache=False
        )
        return out.hidden_states[-1]  # [B,T,H]

    def reward_answer_only(self, input_ids, attention_mask, answer_mask, length_beta: float = 0.0):
        h  = self._last_hidden(input_ids, attention_mask)            # [B,T,H]
        am = (answer_mask.float() * attention_mask.float())          # [B,T]
        denom = am.sum(dim=1, keepdim=True).clamp_min(1.0)
        z = (h * am.unsqueeze(-1)).sum(dim=1) / denom                # [B,H] mean-pool over answer tokens
        z = self.dropout(z)
        s = self.value_head(z).squeeze(-1)                           # [B]
        if length_beta and length_beta > 0:
            s = s - float(length_beta) * torch.log1p(denom.squeeze(1))
        return s

    # you can expose z if you want
    def answer_latent(self, input_ids, attention_mask, answer_mask):
        h  = self._last_hidden(input_ids, attention_mask)
        am = (answer_mask.float() * attention_mask.float())
        denom = am.sum(dim=1, keepdim=True).clamp_min(1.0)
        z = (h * am.unsqueeze(-1)).sum(dim=1) / denom
        return z  # [B,H]

model = LMRewardModel(backbone, hidden_size, mlp_hidden=MLP_HIDDEN, dropout_p=DROPOUT_P).to(device)

# Freeze & optional unfreeze top-N
if FREEZE_BACKBONE:
    for p in model.backbone.parameters(): p.requires_grad = False
    if UNFREEZE_TOP_N > 0:
        try:
            blocks = model.backbone.transformer.h
            for blk in blocks[-UNFREEZE_TOP_N:]:
                for p in blk.parameters(): p.requires_grad = True
            print(f"Unfroze top {UNFREEZE_TOP_N} blocks.")
        except Exception as e:
            print("Unfreeze failed:", e)

# Optimizer param groups
param_groups = [{"params": model.value_head.parameters(), "lr": 1e-4}]
if FREEZE_BACKBONE and UNFREEZE_TOP_N > 0:
    top_params = []
    for n,p in model.named_parameters():
        if p.requires_grad and "transformer.h" in n:
            top_params.append(p)
    if top_params:
        param_groups.append({"params": top_params, "lr": 5e-5})

OPT    = torch.optim.AdamW(param_groups, weight_decay=5e-4)
SCHED  = torch.optim.lr_scheduler.ReduceLROnPlateau(OPT, mode='min', factor=0.5, patience=2)
SCALER = torch.amp.GradScaler("cuda", enabled=(device=="cuda"))
USE_AMP = (device=="cuda")

# A6: Loss & eval
def bt_loss(sp, sn, topk_hard=2):
    base = F.softplus(-(sp - sn)).mean()
    if topk_hard and sn.numel() > 1:
        k = min(topk_hard, sn.size(0))
        topk_vals, _ = torch.topk(sn, k=k, dim=0)
        hard = F.softplus(-(sp.unsqueeze(1) - topk_vals.unsqueeze(0))).mean()
        return 0.5*base + 0.5*hard
    return base

def right_crop_batch(batch, L):
    for k in batch:
        if batch[k].dim() == 2 and batch[k].size(1) > L:
            batch[k] = batch[k][:, -L:]
    return batch

@torch.no_grad()
def eval_pairwise_metrics(model, loader, length_beta=LENGTH_BETA):
    model.eval()
    pos, neg = [], []
    for b in loader:
        b = guard_and_crop_before_forward(b, MODEL_MAXLEN)
        ci, ca, cm = b["ch_ids"].to(device), b["ch_attn"].to(device), b["ch_ansm"].to(device)
        ri, ra, rm = b["rj_ids"].to(device), b["rj_attn"].to(device), b["rj_ansm"].to(device)
        sp = model.reward_answer_only(ci, ca, cm, length_beta).cpu()
        sn = model.reward_answer_only(ri, ra, rm, length_beta).cpu()
        pos.append(sp); neg.append(sn)
    if not pos: return 0., float('nan'), 0., 0.
    sp = torch.cat(pos).numpy(); sn = torch.cat(neg).numpy()
    acc = float((sp > sn).mean())
    sp_c = sp - 0.5*(sp+sn); sn_c = sn - 0.5*(sp+sn)
    y = np.concatenate([np.ones_like(sp_c), np.zeros_like(sn_c)])
    s = 1.0 / (1.0 + np.exp(-np.concatenate([sp_c, sn_c])))
    try: auc = float(roc_auc_score(y, s))
    except Exception: auc = float('nan')
    return acc, auc, float((sp-sn).mean()), float((sp-sn).std())

@torch.no_grad()
def eval_fixed_margin_loss(model, loader, margin=0.2, length_beta=LENGTH_BETA):
    model.eval(); tot=0.0; cnt=0
    for b in loader:
        b = guard_and_crop_before_forward(b, MODEL_MAXLEN)
        ci, ca, cm = b["ch_ids"].to(device), b["ch_attn"].to(device), b["ch_ansm"].to(device)
        ri, ra, rm = b["rj_ids"].to(device), b["rj_attn"].to(device), b["rj_ansm"].to(device)
        sp = model.reward_answer_only(ci, ca, cm, length_beta)
        sn = model.reward_answer_only(ri, ra, rm, length_beta)
        loss = F.margin_ranking_loss(sp, sn, target=torch.ones_like(sp), margin=margin, reduction="mean")
        tot += float(loss.item()) * ci.size(0); cnt += ci.size(0)
    return tot / max(cnt, 1)

# A7: Train loop
best_val = float('inf'); best_auc = -1.0; bad = 0
print("Start RM training...")

for ep in range(1, EPOCHS+1):
    model.train(); run_loss=0.0
    pbar = tqdm(tr_loader, desc=f"Epoch {ep}")
    OPT.zero_grad(set_to_none=True)

    for step, b in enumerate(pbar, start=1):
        b = guard_and_crop_before_forward(b, MODEL_MAXLEN)
        ci, ca, cm = b["ch_ids"].to(device), b["ch_attn"].to(device), b["ch_ansm"].to(device)
        ri, ra, rm = b["rj_ids"].to(device), b["rj_attn"].to(device), b["rj_ansm"].to(device)

        with torch.amp.autocast(device_type="cuda", enabled=USE_AMP):
            sp = model.reward_answer_only(ci, ca, cm, LENGTH_BETA)
            sn = model.reward_answer_only(ri, ra, rm, LENGTH_BETA)
            loss = bt_loss(sp, sn, topk_hard=TOPK_HARD) / GRAD_ACC_STEPS

        SCALER.scale(loss).backward()
        torch.nn.utils.clip_grad_norm_(filter(lambda p: p.requires_grad, model.parameters()), 1.0)
        if step % GRAD_ACC_STEPS == 0:
            SCALER.step(OPT); SCALER.update(); OPT.zero_grad(set_to_none=True)

        run_loss += float(loss.item()) * ci.size(0) * GRAD_ACC_STEPS
        pbar.set_postfix(loss=f"{(loss.item()*GRAD_ACC_STEPS):.4f}")

    tr_acc, tr_auc, tr_dm, tr_ds = eval_pairwise_metrics(model, tr_loader)
    va_acc, va_auc, va_dm, va_ds = eval_pairwise_metrics(model, va_loader)
    va_fixed = eval_fixed_margin_loss(model, va_loader, margin=0.2)
    SCHED.step(va_fixed)
    print(f"Epoch {ep:02d} | train_loss {run_loss/len(tr_loader.dataset):.4f} "
          f"| val_fixed {va_fixed:.4f} | train Acc {tr_acc:.3f} AUC {tr_auc:.3f} (Δ {tr_dm:.3f}±{tr_ds:.3f}) "
          f"| val Acc {va_acc:.3f} AUC {va_auc:.3f} (Δ {va_dm:.3f}±{va_ds:.3f})")

    improved = (va_auc > best_auc + 1e-4) or (va_auc >= best_auc - 1e-4 and va_fixed < best_val - 1e-4)
    if improved:
        best_auc = va_auc; best_val = va_fixed
        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}
        bad = 0
    else:
        bad += 1
        if bad >= PATIENCE:
            print("Early stopping."); break

if 'best_state' in globals():
    model.load_state_dict({k: v.to(device) for k, v in best_state.items()})

tr_acc, tr_auc, tr_dm, tr_ds = eval_pairwise_metrics(model, tr_loader)
va_acc, va_auc, va_dm, va_ds = eval_pairwise_metrics(model, va_loader)
print("\nFinal metrics:")
print(f"Train Acc {tr_acc:.3f} | AUC {tr_auc:.3f} | Δ {tr_dm:.3f}±{tr_ds:.3f}")
print(f"Val   Acc {va_acc:.3f} | AUC {va_auc:.3f} | Δ {va_dm:.3f}±{va_ds:.3f}")

# A8: Export latents (per-answer z_ans; per-pair z_delta) + PCA(64)
@torch.no_grad()
def extract_answer_latents_df(model, split, loader):
    # Returns DataFrame with per-answer rows for chosen & rejected
    model.eval()
    rows = []
    idx = 0
    for b in loader:
        b = guard_and_crop_before_forward(b, MODEL_MAXLEN)
        for which in ("ch","rj"):
            ids = b[f"{which}_ids"].to(device)
            att = b[f"{which}_attn"].to(device)
            am  = b[f"{which}_ansm"].to(device)
            z = model.answer_latent(ids, att, am).cpu().numpy()  # [B,H]
            B, H = z.shape
            for i in range(B):
                row = {"pair_batch_idx": idx + i, "which": ("chosen" if which=="ch" else "rejected")}
                # store as list; we'll keep raw .npy and a PCA CSV
                row["z"] = z[i].tolist()
                rows.append(row)
        idx += b["ch_ids"].size(0)
    return pd.DataFrame(rows)

@torch.no_grad()
def extract_pair_delta_matrix(model, loader):
    # Returns numpy array of z_delta per pair
    model.eval()
    ZD = []
    for b in loader:
        b = guard_and_crop_before_forward(b, MODEL_MAXLEN)
        ci, ca, cm = b["ch_ids"].to(device), b["ch_attn"].to(device), b["ch_ansm"].to(device)
        ri, ra, rm = b["rj_ids"].to(device), b["rj_attn"].to(device), b["rj_ansm"].to(device)
        zc = model.answer_latent(ci, ca, cm)  # [B,H]
        zr = model.answer_latent(ri, ra, rm)  # [B,H]
        zd = (zc - zr).cpu().numpy()
        ZD.append(zd)
    return np.vstack(ZD)

os.makedirs("exports", exist_ok=True)

# Extract per-answer latents on train/val, and per-pair deltas on val
df_z_va = extract_answer_latents_df(model, test_split, va_loader)
Z_delta_val = extract_pair_delta_matrix(model, va_loader)

# Save raw latents as .npy to avoid giant CSV
np.save("exports/val_z_ans_raw.npy", np.vstack(df_z_va["z"].to_numpy()))
np.save("exports/val_z_delta_raw.npy", Z_delta_val)

# PCA(64) for light CSVs
pca = PCA(n_components=64, random_state=SEED)
# Fit PCA on *train* per-answer latents to avoid leakage
df_z_tr = extract_answer_latents_df(model, train_split, tr_loader)
Z_tr = np.vstack(df_z_tr["z"].to_numpy())
pca.fit(Z_tr)

Z_va_64 = pca.transform(np.vstack(df_z_va["z"].to_numpy()))
Z_dv_64 = pca.transform(Z_delta_val)

# Save compact CSVs
pd.DataFrame(Z_va_64).to_csv("exports/val_z_ans_pca64.csv", index=False)
pd.DataFrame(Z_dv_64).to_csv("exports/val_z_delta_pca64.csv", index=False)
df_meta_va = df_z_va.drop(columns=["z"])
df_meta_va.to_csv("exports/val_z_ans_meta.csv", index=False)

print("Saved:")
print(" - exports/val_z_ans_raw.npy")
print(" - exports/val_z_delta_raw.npy")
print(" - exports/val_z_ans_pca64.csv")
print(" - exports/val_z_delta_pca64.csv")
print(" - exports/val_z_ans_meta.csv")

# A9: AutoMetrics scoring for each answer (chosen/rejected)

from transformers import pipeline
from sentence_transformers import SentenceTransformer, util
import textstat

# -- Model cards (for your paper notes) --
# all-MiniLM-L6-v2: https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2
# Intel/polite-guard: https://huggingface.co/Intel/polite-guard
# unitary/toxic-bert: https://huggingface.co/unitary/toxic-bert
# opendatalab/meta-rater-readability-rating: https://huggingface.co/opendatalab/meta-rater-readability-rating
# textstat: https://pypi.org/project/textstat/

# Device mapping for pipelines
pipe_device = 0 if device == "cuda" else -1

# Scorers
emb_model   = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")
polite_clf  = pipeline("text-classification", model="Intel/polite-guard",
                       return_all_scores=True, device=pipe_device, truncation=True)
toxic_clf   = pipeline("text-classification", model="unitary/toxic-bert",
                       return_all_scores=True, device=pipe_device, truncation=True)
read_clf    = pipeline("text-classification", model="opendatalab/meta-rater-readability-rating",
                       return_all_scores=True, device=pipe_device, truncation=True)

def fit_cosine(prompt, answer):
    pe = emb_model.encode(prompt, normalize_embeddings=True)
    ae = emb_model.encode(answer, normalize_embeddings=True)
    return float(util.cos_sim(pe, ae))

def politeness_0_1(text):
    out = polite_clf(text)  # list[list[dict]]
    if isinstance(out, list) and len(out)>0 and isinstance(out[0], list):
        scores = {d["label"].lower(): d["score"] for d in out[0]}
        pos = scores.get("polite", 0.0) + scores.get("respectful", 0.0)
        neg = scores.get("rude", 0.0) + scores.get("offensive", 0.0)
        denom = max(pos + neg, 1e-6)
        return float(pos / denom)
    return float("nan")

def toxicity_0_1(text):
    out = toxic_clf(text)
    if isinstance(out, list) and len(out)>0 and isinstance(out[0], list):
        scores = {d["label"].lower(): d["score"] for d in out[0]}
        if "toxic" in scores:
            return float(scores["toxic"])
        return float(np.mean(list(scores.values())))
    return float("nan")

def readability_model_0_1(text):
    out = read_clf(text)
    if isinstance(out, list) and len(out)>0 and isinstance(out[0], list):
        scores = {d["label"].lower(): d["score"] for d in out[0]}
        # normalize prob(readable) to [0,1]
        if "readable" in scores and "unreadable" in scores:
            total = scores["readable"] + scores["unreadable"]
            return float(scores["readable"] / max(total, 1e-6))
        # fallback: if only one label
        if "readable" in scores:
            return float(scores["readable"])
        if "unreadable" in scores:
            return float(1.0 - scores["unreadable"])
    return float("nan")

def repetition_scores(text, n=1):
    toks = re.findall(r"\w+", text.lower())
    if len(toks)==0: return 0.0
    ngrams = list(zip(*[toks[i:] for i in range(n)]))
    if len(ngrams)==0: return 0.0
    uniq = len(set(ngrams))
    return float(1.0 - uniq/len(ngrams))

def structural_feats(text):
    sents = re.split(r"[.!?]+", text)
    words = re.findall(r"\w+", text)
    n_words = len(words); n_sent = max(1, len([s for s in sents if s.strip()]))
    avg_wps = n_words / n_sent
    ttr = len(set(words)) / max(1, n_words)
    upper_ratio = sum(ch.isupper() for ch in text)/max(1,len(text))
    punct_ratio = sum(ch in ".,;:!?()" for ch in text)/max(1,len(text))
    url_count = len(re.findall(r"https?://", text))
    # classic readability
    fre   = textstat.flesch_reading_ease(text)
    fk    = textstat.flesch_kincaid_grade(text)
    fog   = textstat.gunning_fog(text)
    smog  = textstat.smog_index(text)
    ari   = textstat.automated_readability_index(text)
    return dict(
        n_words=n_words, n_sent=n_sent, avg_words_per_sent=avg_wps, ttr=ttr,
        upper_ratio=upper_ratio, punct_ratio=punct_ratio, url_count=url_count,
        flesch_reading_ease=fre, fk_grade=fk, gunning_fog=fog, smog=smog, ari=ari
    )

def score_pairs(split, n_max=None):
    data = split if n_max is None else split.select(range(min(n_max, len(split))))
    rows=[]
    for ex in tqdm(data, desc="AutoMetrics"):
        p = ex["prompt"]; ch = ex["chosen"]; rj = ex["rejected"]
        for tag, a in (("chosen", ch), ("rejected", rj)):
            row = dict(which=tag)
            row["fit_cosine"]      = fit_cosine(p, a)
            row["politeness_0_1"]  = politeness_0_1(a)
            row["toxicity_0_1"]    = toxicity_0_1(a)
            row["readability_0_1"] = readability_model_0_1(a)
            row["rep_1gram"] = repetition_scores(a, 1)
            row["rep_2gram"] = repetition_scores(a, 2)
            row["rep_3gram"] = repetition_scores(a, 3)
            row.update(structural_feats(a))
            rows.append(row)
    return pd.DataFrame(rows)

os.makedirs("exports", exist_ok=True)
df_val_metrics = score_pairs(test_split, n_max=AUTOMETRICS_MAX)
df_val_metrics.to_csv("exports/val_autometrics.csv", index=False)
print("Saved exports/val_autometrics.csv")

# A9b: LLM-as-a-Judge scoring (JSON rubric -> CSV)
import json, re
from transformers import pipeline

# You can switch to a bigger judge model if VRAM allows:
JUDGE_MODEL = "Qwen/Qwen2.5-1.5B-Instruct"   # e.g., "mistralai/Mistral-7B-Instruct-v0.2"
pipe_device = 0 if device == "cuda" else -1
judge = pipeline("text-generation", model=JUDGE_MODEL, device=pipe_device)

def _truncate_chars(s: str, max_chars: int = 1500):
    if len(s) <= max_chars: return s
    return s[:max_chars]

JUDGE_SYSTEM = (
"You are a strict, fair evaluator. Score the assistant's answer for the given user prompt. "
"Return ONLY a JSON object with 5 float fields in [0,1]: "
"{\"helpfulness\":x, \"harmlessness\":x, \"clarity\":x, \"politeness\":x, \"overall\":x}. "
"Do not add any extra text."
)

def build_judge_prompt(prompt: str, answer: str) -> str:
    prompt = _truncate_chars(prompt, 1500)
    answer = _truncate_chars(answer, 1500)
    return (
f"{JUDGE_SYSTEM}\n\n"
f"User prompt:\n{prompt}\n\n"
f"Assistant answer:\n{answer}\n\n"
"Now output the JSON only."
    )

def parse_json_scores(text: str):
    # Extract first {...} and parse JSON; clamp to [0,1]
    m = re.search(r"\{.*\}", text, flags=re.S)
    if not m: return None
    try:
        obj = json.loads(m.group(0))
        keys = ["helpfulness","harmlessness","clarity","politeness","overall"]
        out = {}
        for k in keys:
            v = float(obj.get(k, 0.0))
            out[k] = float(min(1.0, max(0.0, v)))
        return out
    except Exception:
        return None

def judge_score_single(prompt: str, answer: str, max_new_tokens=128, temperature=0.2, top_p=0.9):
    text_in = build_judge_prompt(prompt, answer)
    out = judge(
        text_in,
        do_sample=True, temperature=temperature, top_p=top_p,
        max_new_tokens=max_new_tokens, pad_token_id=tokenizer.pad_token_id
    )
    # HF pipeline returns list of dicts; text in "generated_text"
    gen = out[0]["generated_text"] if isinstance(out, list) else str(out)
    scores = parse_json_scores(gen)
    if scores is None:
        # fallback: minimal robustness
        scores = {"helpfulness":0.5,"harmlessness":0.5,"clarity":0.5,"politeness":0.5,"overall":0.5}
    return scores

def judge_score_split(split, n_max=None):
    data = split if n_max is None else split.select(range(min(n_max, len(split))))
    rows=[]
    for ex in tqdm(data, desc="LLM-Judge"):
        p = ex["prompt"]; ch = ex["chosen"]; rj = ex["rejected"]
        for tag, a in (("chosen", ch), ("rejected", rj)):
            sc = judge_score_single(p, a)
            sc["which"] = tag
            rows.append(sc)
    return pd.DataFrame(rows)

# Run judge on validation subset (you may set n_max for speed)
n_max_judge = AUTOMETRICS_MAX  # reuse same cap; set None for full
df_val_judge = judge_score_split(test_split, n_max=n_max_judge)
df_val_judge.to_csv("exports/val_llmjudge.csv", index=False)

# Build pair-level deltas (chosen - rejected)
def pair_delta_from_df(df):
    # Assume rows alternate or at least in pairs; safer to group by index
    # We'll simply take order as given: every two rows (chosen, rejected)
    vals = []
    step = 2
    cols = ["helpfulness","harmlessness","clarity","politeness","overall"]
    for i in range(0, len(df), step):
        chunk = df.iloc[i:i+step]
        if len(chunk) < 2: break
        if set(chunk["which"].tolist()) != {"chosen","rejected"}:
            # fallback: skip malformed
            continue
        chosen = chunk[chunk["which"]=="chosen"][cols].iloc[0]
        rejected = chunk[chunk["which"]=="rejected"][cols].iloc[0]
        delta = (chosen - rejected).to_dict()
        vals.append(delta)
    return pd.DataFrame(vals)

df_llmjudge_delta = pair_delta_from_df(df_val_judge)
df_llmjudge_delta.to_csv("exports/val_llmjudge_pair_delta.csv", index=False)

print("Saved:")
print(" - exports/val_llmjudge.csv")
print(" - exports/val_llmjudge_pair_delta.csv")

# =========================
# D0: Setup & Safe Loaders
# =========================
import os, json, math, random, re
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Optional, Tuple, Dict, List
from sklearn.metrics import roc_auc_score
from scipy.stats import pearsonr, spearmanr

np.set_printoptions(suppress=True, linewidth=120)

DATA_DIR = Path("./exports")  # change if needed

FILES = {
    "z_ans_raw": DATA_DIR/"val_z_ans_raw.npy",         # [N_answers, 768]
    "z_delta_raw": DATA_DIR/"val_z_delta_raw.npy",     # [N_pairs, 768]
    "z_ans_pca": DATA_DIR/"val_z_ans_pca64.csv",       # optional
    "z_delta_pca": DATA_DIR/"val_z_delta_pca64.csv",   # optional
    "autometrics": DATA_DIR/"val_autometrics.csv",     # answer-level autometrics
    "judge_ans": DATA_DIR/"val_llmjudge.csv",          # answer-level LLM-judge (has 'which')
    "judge_pair": DATA_DIR/"val_llmjudge_pair_delta.csv", # pair-level Δjudge
}

def exists(p: Path) -> bool:
    return p is not None and Path(p).exists()

def safe_load_npy(p: Path) -> Optional[np.ndarray]:
    if not exists(p):
        print(f"[skip] {p} not found.")
        return None
    try:
        arr = np.load(p)
        print(f"[ok] loaded {p.name} shape={arr.shape}")
        return arr
    except Exception as e:
        print(f"[warn] failed to load {p}: {e}")
        return None

def safe_load_csv(p: Path) -> Optional[pd.DataFrame]:
    if not exists(p):
        print(f"[skip] {p} not found.")
        return None
    try:
        df = pd.read_csv(p)
        print(f"[ok] loaded {p.name} shape={df.shape}")
        return df
    except Exception as e:
        print(f"[warn] failed to load {p}: {e}")
        return None

# Load what we can
Z_ANS = safe_load_npy(FILES["z_ans_raw"])
Z_DELTA = safe_load_npy(FILES["z_delta_raw"])
DF_AUTO = safe_load_csv(FILES["autometrics"])
DF_JUDGE_ANS = safe_load_csv(FILES["judge_ans"])
DF_JUDGE_PAIR = safe_load_csv(FILES["judge_pair"])

# ---------- Pair utilities ----------
def build_pair_index(df: pd.DataFrame) -> pd.Series:
    """
    Build a pair index for answer-level frames.
    Priority:
      1) use 'pair_batch_idx' if provided,
      2) else default to (row_index // 2).
    """
    if df is None:
        return None
    if "pair_batch_idx" in df.columns:
        return df["pair_batch_idx"].astype(int)
    # fallback: assume every two rows form a (chosen, rejected) pair
    idx = np.arange(len(df)) // 2
    return pd.Series(idx, name="pair_batch_idx")

def chosen_mask(df: pd.DataFrame) -> Optional[pd.Series]:
    """
    Return boolean mask for chosen answers if 'which' column exists.
    """
    if df is None:
        return None
    if "which" in df.columns:
        return (df["which"].astype(str).str.lower()=="chosen")
    return None

# Basic sanity alignment (answer-level counts)
if Z_ANS is not None and DF_JUDGE_ANS is not None and len(DF_JUDGE_ANS) != len(Z_ANS):
    print(f"[warn] len(z_ans)={len(Z_ANS)} != len(judge_ans)={len(DF_JUDGE_ANS)}; "
          f"make sure you exported judge on the same subset/order.")

if DF_AUTO is not None and DF_JUDGE_ANS is not None and len(DF_AUTO) != len(DF_JUDGE_ANS):
    print(f"[warn] len(autometrics)={len(DF_AUTO)} != len(judge_ans)={len(DF_JUDGE_ANS)}; "
          f"we will align by row order; ensure same subset/order.")

print("[D0 ready] You can run D1–D3 now when files are available.")

# ==========================================================
# D2 (ROBUST & QUIET): Linear/Sparse Probes from z_ans
# - Skips NaN/near-constant targets
# - Quiet LassoCV with robust settings
# - Minimal, informative prints
# ==========================================================
# from __future__ import annotations
import os, json, warnings
from pathlib import Path
from typing import Dict
import numpy as np
import pandas as pd

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.pipeline import Pipeline
from sklearn.linear_model import RidgeCV, LassoCV
from sklearn.model_selection import KFold
from sklearn.metrics import roc_auc_score
from sklearn.exceptions import ConvergenceWarning
from scipy.stats import pearsonr

# ------------------ resolve upstream globals ------------------
Z_ANS = globals().get("Z_ANS", None)             # np.ndarray [N_answers, 768]
DF_JUDGE_ANS = globals().get("DF_JUDGE_ANS", None)  # pd.DataFrame (per-answer judge)
DF_AUTO = globals().get("DF_AUTO", None)            # pd.DataFrame (per-answer autometrics)
DATA_DIR = Path(globals().get("DATA_DIR", "./artifacts"))
DATA_DIR.mkdir(parents=True, exist_ok=True)
AXES_DIR = DATA_DIR / "axes"
AXES_DIR.mkdir(parents=True, exist_ok=True)

# try to load z if absent
if Z_ANS is None and (DATA_DIR / "val_z_ans_raw.npy").exists():
    Z_ANS = np.load(DATA_DIR / "val_z_ans_raw.npy")

if (Z_ANS is None) or (DF_JUDGE_ANS is None):
    print("[skip] Need Z_ANS and DF_JUDGE_ANS. Make sure previous cells produced them.")
else:
    # ------------------ helpers ------------------
    def pearsonr_safe(a: np.ndarray, b: np.ndarray) -> float:
        a = np.asarray(a); b = np.asarray(b)
        if a.size == 0 or b.size == 0: return np.nan
        if np.allclose(np.std(a), 0.0) or np.allclose(np.std(b), 0.0): return 0.0
        r, _ = pearsonr(a, b)
        return float(r)

    def build_pair_index(df: pd.DataFrame) -> pd.Series:
        for col in ("pair_id","pair","pair_index"):
            if col in df.columns: return df[col].reset_index(drop=True)
        for col in ("is_chosen","chosen","label_is_chosen"):
            if col in df.columns:
                m = df[col].astype(bool).reset_index(drop=True)
                return m.cumsum() - 1
        n = len(df); return pd.Series((np.arange(n)//2).astype(int), index=df.index)

    def chosen_mask(df: pd.DataFrame) -> pd.Series:
        for col in ("is_chosen","chosen","label_is_chosen"):
            if col in df.columns: return df[col].astype(bool).reset_index(drop=True)
        n = len(df); return pd.Series((np.arange(n)%2)==0, index=df.index)

    def pair_auc_from_delta(delta: pd.Series|np.ndarray) -> float:
        d = delta.values if isinstance(delta, pd.Series) else np.asarray(delta)
        y = np.concatenate([np.ones_like(d), np.zeros_like(d)])
        s = np.concatenate([d, -d])
        try:
            return float(roc_auc_score(y, s))
        except Exception:
            return float("nan")

    def pair_metrics_from_answer_preds(yhat: np.ndarray, df_ans: pd.DataFrame) -> Dict[str,float]:
        pidx = build_pair_index(df_ans); chm = chosen_mask(df_ans)
        dfp = pd.DataFrame({"pair": pidx.values, "chosen": chm.values, "yhat": np.asarray(yhat)})
        ch = dfp[dfp["chosen"]].groupby("pair")["yhat"].first()
        rj = dfp[~dfp["chosen"]].groupby("pair")["yhat"].first()
        inter = ch.index.intersection(rj.index)
        if len(inter)==0: return {"pair_agree": np.nan, "pair_auc": np.nan, "n_pairs": 0}
        delta = ch.loc[inter] - rj.loc[inter]
        return {
            "pair_agree": float((delta.values > 0).mean()),
            "pair_auc": pair_auc_from_delta(delta),
            "n_pairs": int(len(delta))
        }

    # ------------------ align lengths & merge targets ------------------
    N, D = Z_ANS.shape
    DF_JUDGE_ANS = DF_JUDGE_ANS.reset_index(drop=True)
    if len(DF_JUDGE_ANS) != N:
        n_min = min(N, len(DF_JUDGE_ANS))
        print(f"[warn] Length mismatch: Z_ANS={N}, DF_JUDGE_ANS={len(DF_JUDGE_ANS)} -> truncate to {n_min}")
        Z_ANS = Z_ANS[:n_min]; DF_JUDGE_ANS = DF_JUDGE_ANS.iloc[:n_min].reset_index(drop=True); N = n_min

    if DF_AUTO is not None:
        DF_AUTO = DF_AUTO.reset_index(drop=True)
        if len(DF_AUTO) != N:
            n_min = min(N, len(DF_AUTO))
            print(f"[warn] Length mismatch: DF_AUTO={len(DF_AUTO)} -> truncate to {n_min}")
            DF_AUTO = DF_AUTO.iloc[:n_min].reset_index(drop=True)

    df_targets = DF_JUDGE_ANS.copy()
    if DF_AUTO is not None:
        for c in DF_AUTO.columns:
            if c not in df_targets.columns:
                df_targets[c] = DF_AUTO[c].values

    wanted = [
        "overall","helpfulness","harmlessness","clarity","politeness",
        "readability_0_1","politeness_0_1","toxicity_0_1","fit_cosine",
        "c_flesch_reading_ease","c_fk_grade","c_entropy_norm",
        "c_rep_1gram","c_rep_2gram","c_rep_3gram"
    ]
    target_cols = [c for c in wanted if c in df_targets.columns]
    if not target_cols:
        print("[skip] No valid target columns in df_targets.")
    else:
        print(f"[info] Z_ANS: {Z_ANS.shape} | candidate targets: {len(target_cols)}")

        # ------------------ probe configs ------------------
        k_values = [16, 32, 64, 128]
        cv = KFold(n_splits=5, shuffle=True, random_state=42)
        results = []

        # global warning control: quiet convergence spam
        warnings.filterwarnings("ignore", category=ConvergenceWarning)

        for tgt in target_cols:
            y_raw = pd.to_numeric(df_targets[tgt], errors="coerce").values.astype(float)
            X_raw = Z_ANS.copy()

            # drop NaN/inf rows for this target
            mask = np.isfinite(y_raw)
            if mask.sum() < max(50, int(0.2*N)):
                print(f"[skip:{tgt}] too few finite samples ({mask.sum()}/{N}).")
                continue
            y = y_raw[mask]; X = X_raw[mask]

            std_y = float(np.std(y))
            if std_y < 1e-8:
                print(f"[skip:{tgt}] near-constant target (std={std_y:.2e}).")
                continue

            # (A) Ridge + PCA (pick best k by Pearson r on full masked set)
            bestA = {"k": None, "r": -np.inf, "r2": np.nan, "pair_auc": np.nan, "pair_agree": np.nan}
            bestA_model = None
            for k in k_values:
                try:
                    pipe = Pipeline([
                        ("scaler", StandardScaler()),
                        ("pca", PCA(n_components=k, random_state=42)),
                        ("reg", RidgeCV(alphas=np.logspace(-3,3,13), cv=cv))
                    ])
                    pipe.fit(X, y)
                    yhat = pipe.predict(X)
                    r = pearsonr_safe(y, yhat)
                    r2 = r*r if np.isfinite(r) else np.nan
                    pdm = pair_metrics_from_answer_preds(yhat, df_targets.iloc[mask.nonzero()[0]])
                    if r > bestA["r"]:
                        bestA = {"k": k, "r": r, "r2": r2,
                                 "pair_auc": float(pdm["pair_auc"]), "pair_agree": float(pdm["pair_agree"])}
                        bestA_model = pipe
                except Exception as e:
                    print(f"[warn:{tgt}] Ridge+PCA k={k} failed: {e}")

            # (B) Lasso on raw 768-d (sparse, interpretable)
            try:
                with warnings.catch_warnings():
                    warnings.simplefilter("ignore", category=ConvergenceWarning)
                    pipeB = Pipeline([
                        ("scaler", StandardScaler(with_mean=True, with_std=True)),
                        ("lasso", LassoCV(
                            alphas=np.logspace(-3, 1, 30),
                            cv=cv, random_state=42, max_iter=50000, tol=1e-4, selection="cyclic"
                        ))
                    ])
                    pipeB.fit(X, y)
                    yhatB = pipeB.predict(X)
                    rB = pearsonr_safe(y, yhatB)
                    r2B = rB*rB if np.isfinite(rB) else np.nan
                    pdmB = pair_metrics_from_answer_preds(yhatB, df_targets.iloc[mask.nonzero()[0]])

                    scaler = pipeB.named_steps["scaler"]
                    lasso  = pipeB.named_steps["lasso"]
                    scale = np.where(scaler.scale_==0, 1.0, scaler.scale_)
                    axis_w = lasso.coef_ / scale
                    axis_b = float(lasso.intercept_ - np.dot(scaler.mean_/scale, lasso.coef_))

                    np.save(AXES_DIR / f"axis_{tgt}_lasso_weights.npy", axis_w.astype(np.float32))
                    with open(AXES_DIR / f"axis_{tgt}_lasso_meta.json", "w") as f:
                        json.dump({"bias": axis_b, "target": tgt, "desc": "Lasso axis in original z_ans space"}, f)

                    results.append({
                        "target": tgt,
                        "ridge_k": bestA["k"], "ridge_r": bestA["r"], "ridge_r2": bestA["r2"],
                        "ridge_pair_auc": bestA["pair_auc"], "ridge_pair_agree": bestA["pair_agree"],
                        "lasso_r": rB, "lasso_r2": r2B,
                        "lasso_pair_auc": float(pdmB["pair_auc"]), "lasso_pair_agree": float(pdmB["pair_agree"]),
                        "lasso_nnz": int(np.sum(np.abs(axis_w) > 1e-8)),
                        "n_used": int(len(y))
                    })
            except Exception as e:
                print(f"[warn:{tgt}] Lasso failed: {e}")

        if results:
            df_probe = pd.DataFrame(results).sort_values(
                ["lasso_pair_auc","ridge_pair_auc","lasso_r","ridge_r"], ascending=False
            )
            out_csv = DATA_DIR / "probe_results.csv"
            df_probe.to_csv(out_csv, index=False)
            print(f"[ok] saved probe results -> {out_csv}")
            try:
                from IPython.display import display
                display(df_probe.head(10))
            except Exception:
                print(df_probe.head(10))
        else:
            print("[warn] No probe results produced. Check targets and Z_ANS.")

# ===== Sanity & Alignment Check (non-destructive) =====
import numpy as np, pandas as pd
from pathlib import Path
from scipy.stats import pearsonr

Z = globals().get("Z_ANS", None)                  # np.ndarray [N_ans, 768]
DFJ = globals().get("DF_JUDGE_ANS", None)         # per-answer judge df
DFA = globals().get("DF_AUTO", None)              # per-answer autometrics df (optional)
DATA_DIR = Path(globals().get("DATA_DIR", "./artifacts"))

def _chosen_mask(df):
    for c in ["is_chosen","chosen","label_is_chosen"]:
        if c in df.columns: return df[c].astype(bool).reset_index(drop=True)
    # fallback: even rows are chosen
    n = len(df)
    return pd.Series((np.arange(n)%2)==0, index=df.index)

def _pair_index(df):
    for c in ["pair_id","pair","pair_index"]:
        if c in df.columns: return df[c].reset_index(drop=True)
    # fallback: consecutive answers per pair (0/1, 2/3, ...)
    n = len(df)
    return pd.Series((np.arange(n)//2).astype(int), index=df.index)

def _nan_ratio(x):
    x = pd.to_numeric(pd.Series(x), errors="coerce")
    return float(np.mean(~np.isfinite(x)))

def _std_safe(x):
    x = pd.to_numeric(pd.Series(x), errors="coerce")
    x = x[np.isfinite(x)]
    return float(np.std(x)) if len(x) else 0.0

problems = []

if Z is None or DFJ is None:
    raise RuntimeError("Need Z_ANS and DF_JUDGE_ANS in memory.")

N, D = Z.shape
DFJ = DFJ.reset_index(drop=True).copy()
if len(DFJ) != N:
    nmin = min(N, len(DFJ))
    print(f"[warn] length mismatch: Z_ANS={N}, DF_JUDGE_ANS={len(DFJ)} -> truncate to {nmin}")
    Z = Z[:nmin]
    DFJ = DFJ.iloc[:nmin].reset_index(drop=True)
    N = nmin

if DFA is not None:
    DFA = DFA.reset_index(drop=True).copy()
    if len(DFA) != N:
        nmin = min(N, len(DFA))
        print(f"[warn] autometrics length mismatch: DF_AUTO={len(DFA)} -> truncate to {nmin}")
        DFA = DFA.iloc[:nmin].reset_index(drop=True)

# 1) Pair structure sanity
pair = _pair_index(DFJ)
chosen = _chosen_mask(DFJ)
per_pair_count = pair.value_counts().describe()
pair_ok_ratio = float((pair.value_counts()==2).mean())
print(f"[pair] N_answers={N}, N_pairs≈{pair.nunique()}, pairs with 2 answers ratio={pair_ok_ratio:.3f}")
if pair_ok_ratio < 0.98:
    problems.append("Some pairs don't have exactly two answers after filtering.")

# 2) Chosen/rejected alternation sanity (only if no explicit column)
alt_ok = None
if not any(c in DFJ.columns for c in ["is_chosen","chosen","label_is_chosen"]):
    alt_ok = bool(np.all((np.arange(N)%2==0) == chosen.values))
    print(f"[pair] fallback alternating chosen mask ok? {alt_ok}")
    if not alt_ok:
        problems.append("Fallback (even-index=chosen) seems invalid. Need explicit chosen column.")

# 3) Target columns sanity
wanted = [
    "overall","helpfulness","harmlessness","clarity","politeness",
    "readability_0_1","politeness_0_1","toxicity_0_1","fit_cosine",
    "c_flesch_reading_ease","c_fk_grade","c_entropy_norm",
    "c_rep_1gram","c_rep_2gram","c_rep_3gram"
]
df_targets = DFJ.copy()
if DFA is not None:
    for c in DFA.columns:
        if c not in df_targets.columns:
            df_targets[c] = DFA[c].values

present = [c for c in wanted if c in df_targets.columns]
print(f"[targets] found {len(present)} targets:", present)

# stats
rows = []
for c in present:
    nanr = _nan_ratio(df_targets[c])
    stdc = _std_safe(df_targets[c])
    rows.append({"target": c, "nan_ratio": nanr, "std": stdc})
df_tstat = pd.DataFrame(rows).sort_values(["nan_ratio","std"], ascending=[False, True])
print("\n[target quality]\n", df_tstat.to_string(index=False))

# 4) Build a CLEAN alignment view (only rows with finite targets for at least one dim)
finite_any = np.zeros(N, dtype=bool)
for c in present:
    vals = pd.to_numeric(df_targets[c], errors="coerce").values
    finite_any |= np.isfinite(vals)

clean = pd.DataFrame({
    "pair": pair.values,
    "chosen": chosen.values,
})
clean["finite_any_target"] = finite_any
clean["keep_pair_both"] = clean.groupby("pair")["finite_any_target"].transform(lambda s: s.sum()==2)
keep_mask = clean["keep_pair_both"].values

n_keep = int(keep_mask.sum())
print(f"\n[clean] answers kept={n_keep}/{N} ({n_keep/N:.1%}); "
      f"pairs kept={int(clean[keep_mask]['pair'].nunique())}/{clean['pair'].nunique()}")

# 5) Save a manifest (optional, helps downstream joins)
manifest = clean.copy()
manifest["row_idx"] = np.arange(N)
if "prompt" in DFJ.columns: manifest["has_prompt"] = True
DATA_DIR.mkdir(parents=True, exist_ok=True)
manifest_path = DATA_DIR / "val_alignment_manifest.csv"
manifest.to_csv(manifest_path, index=False)
print(f"[ok] wrote manifest -> {manifest_path}")

# 6) Quick diagnosis summary
if problems:
    print("\n[DIAGNOSIS]")
    for p in problems:
        print(" -", p)
else:
    print("\n[DIAGNOSIS] no structural red flags; main issues likely NaN/near-constant targets.")

# # ==== Rebuild AutoMetrics (answer-level) & Clean Judge Stub ====
# import numpy as np, pandas as pd, torch, re
# from pathlib import Path
# from tqdm.auto import tqdm

# # 1) Load alignment manifest to guarantee same ordering as Z_ANS
# DATA_DIR = Path(globals().get("DATA_DIR", "./exports"))
# man_path = DATA_DIR / "val_alignment_manifest.csv"
# manifest = pd.read_csv(man_path)
# assert "row_idx" in manifest.columns and "pair" in manifest.columns and "chosen" in manifest.columns
# N = len(manifest)
# print(f"[manifest] rows={N}, pairs={manifest['pair'].nunique()}, chosen_ratio={manifest['chosen'].mean():.3f}")

# # 2) Rebuild raw texts in EXACT same order as z_ans
# #    Assumes your val split is the same 600 pairs used to export Z_ANS (=1200 answers).
# #    We reconstruct by iterating test_split and pushing [chosen, rejected] for each pair.
# val_prompts, val_answers = [], []
# for i in range(len(test_split)):
#     pr = test_split[i]["prompt"]
#     ch = test_split[i]["chosen"]
#     rj = test_split[i]["rejected"]
#     val_prompts += [pr, pr]
#     val_answers += [ch, rj]

# assert len(val_answers) >= N, "[error] val answers smaller than manifest—did you change TEST_SIZE?"
# val_prompts = val_prompts[:N]; val_answers = val_answers[:N]

# # 3) Fit-to-prompt cosine via sentence-transformers/all-MiniLM-L6-v2
# from sentence_transformers import SentenceTransformer, util
# st = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2", device="cuda" if torch.cuda.is_available() else "cpu")
# with torch.inference_mode():
#     emb_p = st.encode(val_prompts, batch_size=64, convert_to_tensor=True, normalize_embeddings=True)
#     emb_a = st.encode(val_answers, batch_size=64, convert_to_tensor=True, normalize_embeddings=True)
# fit_cosine = util.cos_sim(emb_a, emb_p).diagonal().cpu().numpy()

# # 4) Toxicity via unitary/toxic-bert (multi-label -> average)
# from transformers import AutoTokenizer, AutoModelForSequenceClassification, TextClassificationPipeline
# tox_tok = AutoTokenizer.from_pretrained("unitary/toxic-bert")
# tox_mod = AutoModelForSequenceClassification.from_pretrained("unitary/toxic-bert")
# tox_pipe = TextClassificationPipeline(model=tox_mod, tokenizer=tox_tok, device=0 if torch.cuda.is_available() else -1, return_all_scores=True)
# tox_scores = []
# for i in tqdm(range(0, N, 64), desc="toxicity"):
#     batch = val_answers[i:i+64]
#     out = tox_pipe(batch, truncation=True)
#     # out: list of list[{'label':..., 'score':...}]
#     for labset in out:
#         tox_scores.append(float(np.mean([x["score"] for x in labset])))
# toxicity_0_1 = np.array(tox_scores, dtype=np.float32)

# # 5) Politeness via Intel/polite-guard (4-class -> map to 0..1)
# #    Map: impolite=0, some-impolite=0.33, neutral=0.66, polite=1.0  (you can tweak mapping later)
# pol_tok = AutoTokenizer.from_pretrained("Intel/polite-guard")
# pol_mod = AutoModelForSequenceClassification.from_pretrained("Intel/polite-guard")
# pol_pipe = TextClassificationPipeline(model=pol_mod, tokenizer=pol_tok, device=0 if torch.cuda.is_available() else -1, return_all_scores=False)
# map_pol = {"impolite":0.0, "somewhat_impolite":0.33, "neutral":0.66, "polite":1.0}
# pol_vals = []
# for i in tqdm(range(0, N, 64), desc="politeness"):
#     batch = val_answers[i:i+64]
#     out = pol_pipe(batch, truncation=True)
#     for o in out:
#         lbl = o["label"].lower().replace(" ", "_")
#         pol_vals.append(map_pol.get(lbl, 0.5))
# politeness_0_1 = np.array(pol_vals, dtype=np.float32)

# # 6) Readability (model-based可等你上 server；这里先用 textstat 代理并归一化到[0,1])
# import textstat
# def readability_01(txt: str):
#     try:
#         fre = textstat.flesch_reading_ease(txt)  # ~(-30..120+)
#         fre = max(-30.0, min(120.0, float(fre)))
#         return (fre + 30.0) / 150.0  # -30->0, 120->1
#     except Exception:
#         return np.nan
# read_vals = [readability_01(t) for t in val_answers]
# readability_0_1 = np.array(read_vals, dtype=np.float32)

# # 7) Simple repetition metrics (optional but useful)
# def uniq_ratio(tokens, n=1):
#     if n == 1:
#         return len(tokens) / max(1,len(tokens))
#     return len({tuple(tokens[i:i+n]) for i in range(max(0, len(tokens)-n+1))}) / max(1, len(tokens)-n+1)
# rep1, rep2, rep3 = [], [], []
# for s in val_answers:
#     toks = re.findall(r"\w+|\S", s)
#     rep1.append(1.0 - uniq_ratio(toks, 1))
#     rep2.append(1.0 - uniq_ratio(toks, 2))
#     rep3.append(1.0 - uniq_ratio(toks, 3))
# rep_1gram = np.array(rep1, dtype=np.float32)
# rep_2gram = np.array(rep2, dtype=np.float32)
# rep_3gram = np.array(rep3, dtype=np.float32)

# # 8) Assemble DF_AUTO aligned to manifest row order
# DF_AUTO = pd.DataFrame({
#     "pair": manifest["pair"].values,
#     "is_chosen": manifest["chosen"].astype(bool).values,
#     "fit_cosine": fit_cosine,
#     "toxicity_0_1": toxicity_0_1,
#     "politeness_0_1": politeness_0_1,
#     "readability_0_1": readability_0_1,
#     "c_rep_1gram": rep_1gram,
#     "c_rep_2gram": rep_2gram,
#     "c_rep_3gram": rep_3gram,
# })
# auto_path = DATA_DIR / "val_autometrics.csv"
# DF_AUTO.to_csv(auto_path, index=False)
# print(f"[ok] wrote AutoMetrics -> {auto_path}")

# # 9) Provide a MINIMAL judge stub (avoid constant dummy columns)
# #    We only keep columns needed for pair alignment. Real judge will be added later.
# DF_JUDGE_ANS = manifest[["pair","chosen"]].rename(columns={"chosen":"is_chosen"}).copy()
# print("[ok] DF_JUDGE_ANS stub ready (pair/is_chosen only). Now re-run your Probe (D2) cell.")

# =======================================================
# D3: Select Top-K axes and export a vector-reward sketch
# =======================================================
TOPK = 4  # choose how many attributes to keep as initial axes

probe_csv = DATA_DIR/"probe_results.csv"
if not probe_csv.exists():
    print("[skip] run D2 first.")
else:
    df = pd.read_csv(probe_csv)
    # Ranking priority: pair AUC (lasso), then ridge as tie-break
    df = df.sort_values(["lasso_pair_auc","ridge_pair_auc"], ascending=False)
    sel = df.head(TOPK).copy()
    print("[info] Selected axes:")
    display(sel[["target","lasso_pair_auc","lasso_pair_agree","lasso_r","lasso_nnz","ridge_k","ridge_pair_auc"]])

    axes = []
    meta = []
    for tgt in sel["target"]:
        w = np.load(DATA_DIR/f"axes/axis_{tgt}_lasso_weights.npy")
        axes.append(w)
        meta.append({"name": tgt, "source": "lasso", "dim": len(w)})
    AX = np.vstack(axes)  # [K, 768]
    np.save(DATA_DIR/"vector_reward_axes.npy", AX.astype(np.float32))
    with open(DATA_DIR/"vector_reward_axes_meta.json","w") as f:
        json.dump({"axes": meta, "note": "Top-K axes from lasso on z_ans"}, f, indent=2)
    print(f"[ok] exported {len(axes)} axes -> vector_reward_axes.npy + meta json")

    # Give an initial weight suggestion proportional to pair-AUC
    weights = (sel["lasso_pair_auc"] - 0.5).clip(lower=0)  # baseline 0.5
    if weights.sum() == 0:
        w0 = np.ones(len(weights))/len(weights)
    else:
        w0 = weights / weights.sum()
    np.save(DATA_DIR/"vector_reward_weights_init.npy", w0.values.astype(np.float32))
    print("[ok] saved initial weights -> vector_reward_weights_init.npy")
    print("Weights (sum=1):", w0.values)

print("[D3 done]")
